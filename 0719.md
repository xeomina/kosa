# 0719

## 리뷰

![image-20220719090753541](md-images/0719/image-20220719090753541.png)

![image-20220719091625781](../../../AppData/Roaming/Typora/typora-user-images/image-20220719091625781.png)

* ingress = L7 = ALB - 경로기반 라우팅



* persistentVolumeReclaimPolicy: Retain
  * pod : persistant volume (pvc) - pv? 이용해 연결... > 지울때 pod의 데이터
  * public cloud / openstack 에서는 `delete` 가능 but 쿠버네티스에서는 불가
  * `recycle` : 온프레미스 / vm에서는 delete와 유사한 기능..재활용



![image-20220719094318642](../../../AppData/Roaming/Typora/typora-user-images/image-20220719094318642.png)



## Volume

* 모든 node에서 실행

```
# yum install -y nfs-utils.x86_64	# mount -t nfs
```



* master

```
# mkdir /nfs_shared
# chmod 777 /nfs_shared/
# echo '/nfs_shared 192.168.0.0/20(rw,sync,no_root_squash)' >> /etc/exports
# cat /etc/exports
```

![image-20220719100808409](md-images/0719/image-20220719100808409.png)

![image-20220719101932006](md-images/0719/image-20220719101932006.png)

```
# systemctl enable --now nfs
```

* worker 

```
# mount -t nfs 192.168.1.192:/nfs_shared /mnt
# df -h
# umount /mnt		# unmount
```



### pv

* master node

```
# mkdir nfs-pv-pvc-pod && cd $_
# vi nfs-pv.yaml
apiVersion: v1
kind: PersistentVolume		# pod가 아닌 object
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteMany	# RWX
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    server: 192.168.1.192
    path: /nfs_shared
```

![image-20220719102143309](md-images/0719/image-20220719102143309.png)

```
# kubectl apply -f nfs-pv.yaml
# kubectl get pv
```

![image-20220719102612175](md-images/0719/image-20220719102612175.png)



### pvc

* label 및 selector 없음..어떻게 연결?
  * 후순위로 storage로 연결 - 유사한 크기

```
# vi nfs-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim		# pvc
metadata:
  name: nfs-pvc
spec:
  accessModes:
    - ReadWriteMany				# rwx
  resources:
    requests:
      storage: 100Mi
```

![image-20220719102557230](md-images/0719/image-20220719102557230.png)

```
# kubectl apply -f nfs-pvc.yaml
# kubectl get pv,pvc
```

* STATUS : Bound로 변경됨

![image-20220719102951574](md-images/0719/image-20220719102951574.png)

### Deployment

* replicas: 4
  * 자가 치유...? > 내용 api 전달  > kube 스케쥴러가 리소스 넉넉한 node 스케쥴림 : 갯수 유지

```
# vi nfs-pvc-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-pvc-deploy
spec:
  replicas: 4		# spot 스폿
  selector:
    matchLabels:
      app: nfs-pvc-deploy

  template:
    metadata:
      labels:
        app: nfs-pvc-deploy
    spec:
      containers:
      - name: nginx
        image: nginx
        volumeMounts:
        - name: nfs-vol
          mountPath: /usr/share/nginx/html
      volumes:
      - name: nfs-vol
        persistentVolumeClaim:
          claimName: nfs-pvc
```

![image-20220719103821853](md-images/0719/image-20220719103821853.png)

![image-20220719103836806](md-images/0719/image-20220719103836806.png)

```
# kubectl apply -f nfs-pvc-deploy.yaml
# kubectl get all
```

![image-20220719103949097](md-images/0719/image-20220719103949097.png)

![image-20220719104115742](md-images/0719/image-20220719104115742.png)

* worker1

```
# df -h
```

![image-20220719104233231](md-images/0719/image-20220719104233231.png)

* worker2

```
# df -h
```

![image-20220719104332207](md-images/0719/image-20220719104332207.png)

### Loadbalancer

```
# kubectl expose deployment nfs-pvc-deploy --type=LoadBalancer --name=nfs-pvc-deploy-svc1 --external-ip 192.168.1.192 --port=80
# kubectl get svc
# curl 192.168.1.192
```

![image-20220719104927047](md-images/0719/image-20220719104927047.png)

```
# echo "HELLO" > /nfs_shared/index.html
# curl 192.168.1.192
```

![image-20220719110224599](md-images/0719/image-20220719110224599.png)

### 삭제

```
# kubectl delete deployments.apps nfs-pvc-deploy
# kubectl get all
# kubectl delete pvc nfs-pvc
# kubectl get all
```

* 시간 텀 필요

![image-20220719110728324](md-images/0719/image-20220719110728324.png)

![image-20220719110918322](md-images/0719/image-20220719110918322.png)

* worker

![image-20220719110742982](md-images/0719/image-20220719110742982.png)

![image-20220719110802743](md-images/0719/image-20220719110802743.png)



## VM 환경 설정

* NAT 확인

![image-20220719111708372](md-images/0719/image-20220719111708372.png)

![image-20220719111735631](md-images/0719/image-20220719111735631.png)

* ip 충돌 방지
  * 모든  VM
  * 네트워크 어댑터 1 : NAT 네트워크
  * 네트워크 어댑터 2 : 호스트 전용 어댑터

![image-20220719111904199](md-images/0719/image-20220719111904199.png)

![image-20220719111920706](md-images/0719/image-20220719111920706.png)

### ip 확인

* master 1 : 192.168.56.105
* worker 1 : 192.168.56.106
* worker 2 : 192.168.56.107

## 

* 하나의 pod에 multi container